# LLM Tool Orchestration System

## Overview

This system enables your LLM to **autonomously discover, select, and chain security tools** based on user requests. Inspired by ClatScope's tool aggregation patterns, but with LLM-driven orchestration for true autonomy.

## Architecture

```
User Request
    â†“
LLM Analysis
    â†“
Tool Registry Query â†’ Discover available tools
    â†“
Tool Selection â†’ Select relevant tools based on:
    - User intent
    - Input type (IP, domain, email, etc.)
    - Tool prerequisites
    - Tool chains
    â†“
Orchestration Plan
    â†“
Sequential/Parallel Execution
    â†“
Result Aggregation
    â†“
LLM Synthesis â†’ Final security analysis report
```

## Key Components

### 1. Tool Registry (`tools/tool_registry.py`)

**Purpose**: Central catalog of all available security tools with rich metadata.

**Key Classes**:
- `ToolMetadata`: Describes a tool (name, parameters, category, use cases, etc.)
- `ToolRegistry`: Manages tool registration and discovery
- `InputType`: Enum of input types (IP, domain, email, etc.)
- `ToolCategory`: Enum of tool categories (network, vulnerability, threat intel, etc.)

**Features**:
- Tools self-register with metadata
- LLM-friendly catalog generation
- Search by category, input type, or keyword
- Tool chain recommendations

### 2. Tool Registration (`tools/register_existing_tools.py`)

**Purpose**: Registers all your existing tools (nmap, masscan, shodan, etc.) with metadata.

**Registered Tools**:
- Nmap (Quick Scan, Service Detection, Vuln Scan, Batch Scan)
- Masscan (Batch Scan)
- Naabu (Port Scan)
- Shodan (Threat Intel Lookup)
- Amass (Subdomain Enumeration)
- BBOT (Web Reconnaissance)
- DNS Tools (Resolution, Reverse Lookup)

### 3. LLM Orchestrator (`llm_tool_orchestrator.py`)

**Purpose**: Enables LLM to autonomously orchestrate tool execution.

**Key Classes**:
- `LLMToolOrchestrator`: Main orchestration engine
- `OrchestrationPlan`: Execution plan generated by LLM
- `ToolExecutionResult`: Result of tool execution

**Features**:
- Tool catalog generation for LLM context
- Tool suggestion based on input type
- Tool chain recommendation
- Sequential/parallel execution
- Result aggregation
- LLM synthesis prompts

## Usage Examples

### Example 1: Basic Tool Discovery

```python
from llm_tool_orchestrator import LLMToolOrchestrator
from tools.tool_registry import InputType

# Initialize orchestrator
orchestrator = LLMToolOrchestrator()

# Get tool catalog for LLM
catalog = orchestrator.get_tool_catalog_for_llm()
print(catalog)
```

**Output**: Markdown-formatted catalog of all 15+ tools

### Example 2: Tool Suggestion for Input Type

```python
# User provides an IP address
ip_tools = orchestrator.suggest_tools_for_input("8.8.8.8", InputType.IP_ADDRESS)

for tool in ip_tools:
    print(f"{tool.name}: {tool.description}")
```

**Output**:
```
Nmap Quick Scan: Fast Nmap scan (-T4 -F) to quickly discover open ports
Shodan Lookup: Query Shodan for host information and vulnerabilities
Reverse DNS Lookup: Find hostname from IP address
...
```

### Example 3: Tool Chain Recommendation

```python
# Get recommended chain starting from Nmap Quick Scan
chain = orchestrator.suggest_tool_chain("Nmap Quick Scan")
print(" â†’ ".join(chain))
```

**Output**:
```
Nmap Quick Scan â†’ Nmap Service Detection â†’ Nmap Vulnerability Scan â†’ Shodan Lookup
```

### Example 4: LLM-Driven Tool Selection

```python
# Generate prompt for LLM to select tools
prompt = orchestrator.generate_llm_tool_selection_prompt(
    user_request="Scan example.com for vulnerabilities",
    available_data={"domain": "example.com"}
)

# Send to LLM
llm_response = your_llm_client.generate(prompt)
# LLM returns JSON with selected tools and execution order
```

**LLM Response**:
```json
{
  "selected_tools": [
    {
      "tool_name": "DNS Resolution",
      "purpose": "Convert domain to IP address",
      "parameters": {"domain": "example.com"}
    },
    {
      "tool_name": "Nmap Service Detection",
      "purpose": "Identify running services and versions",
      "parameters": {"target": "<IP from DNS>"}
    },
    {
      "tool_name": "Nmap Vulnerability Scan",
      "purpose": "Check for known vulnerabilities",
      "parameters": {"target": "<IP from DNS>"}
    }
  ],
  "execution_order": ["DNS Resolution", "Nmap Service Detection", "Nmap Vulnerability Scan"],
  "reasoning": "First resolve domain to IP, then identify services, finally scan for vulnerabilities"
}
```

### Example 5: Execute Tool Chain

```python
from llm_tool_orchestrator import create_orchestration_plan_from_llm_response

# Parse LLM response into plan
plan = create_orchestration_plan_from_llm_response(llm_response)

# Execute plan
results = orchestrator.execute_tool_chain(
    plan=plan,
    initial_data={"domain": "example.com"}
)

# Results contain outputs from all tools
for result in results:
    if result.success:
        print(f"âœ“ {result.tool_name} completed in {result.duration:.2f}s")
        print(f"  Output: {result.output}")
    else:
        print(f"âœ— {result.tool_name} failed: {result.error}")
```

### Example 6: LLM Result Synthesis

```python
# Generate synthesis prompt for LLM
synthesis_prompt = orchestrator.generate_llm_synthesis_prompt(
    results=results,
    user_request="Scan example.com for vulnerabilities"
)

# Send to LLM for final report
final_report = your_llm_client.generate(synthesis_prompt)
print(final_report)
```

**LLM Synthesized Report**:
```
# Security Analysis Report: example.com

## Executive Summary
Scanned example.com (resolved to 93.184.216.34) and discovered 3 open ports
with 2 high-severity vulnerabilities requiring immediate attention.

## Attack Surface
- Open Ports: 80/HTTP, 443/HTTPS, 22/SSH
- Services: Apache 2.4.41, OpenSSH 7.6p1
- Technologies: PHP 7.4, MySQL 5.7

## Critical Findings
ðŸ”´ CVE-2021-41773: Apache 2.4.41 Path Traversal (CVSS 9.8)
   - Allows remote code execution
   - Public exploits available
   - IMMEDIATE PATCHING REQUIRED

## Recommendations
1. [URGENT] Upgrade Apache to 2.4.51+ within 24 hours
2. [HIGH] Update OpenSSH to latest version
3. [MEDIUM] Implement WAF rules
...
```

## Integration with Your Agent

### Option 1: Add to Phase 1 (Tool Selection)

Modify `agent.py` Phase 1 to include tool orchestration:

```python
def phase_1_tool_selection(self, user_prompt: str):
    # Initialize orchestrator
    from llm_tool_orchestrator import LLMToolOrchestrator
    orchestrator = LLMToolOrchestrator(llm_client=self)

    # Get tool catalog
    tool_catalog = orchestrator.get_tool_catalog_for_llm()

    # Generate LLM prompt with tool catalog
    prompt = orchestrator.generate_llm_tool_selection_prompt(
        user_request=user_prompt,
        available_data=self._extract_targets(user_prompt)
    )

    # LLM selects tools
    response = self._call_ollama(prompt)

    # Parse and return orchestration plan
    return create_orchestration_plan_from_llm_response(response)
```

### Option 2: New "AutoScan" Meta-Tool

Create a new tool that uses orchestration:

```python
@register_tool(ToolMetadata(
    name="AutoScan",
    function_name="run_autoscan",
    description="LLM-driven automated security scanning with intelligent tool selection",
    category=ToolCategory.OSINT,
    parameters=[
        ToolParameter(
            name="target",
            type=InputType.ANY,
            description="Target (IP, domain, email, etc.)",
            examples=["example.com", "8.8.8.8"]
        ),
        ToolParameter(
            name="intent",
            type=InputType.TEXT,
            description="What to look for (vulnerabilities, subdomains, etc.)",
            examples=["find vulnerabilities", "enumerate attack surface"]
        )
    ],
    ...
))
def run_autoscan(target: str, intent: str) -> dict:
    orchestrator = LLMToolOrchestrator()

    # LLM selects tools
    prompt = orchestrator.generate_llm_tool_selection_prompt(
        user_request=intent,
        available_data={"target": target}
    )

    llm_response = call_llm(prompt)
    plan = create_orchestration_plan_from_llm_response(llm_response)

    # Execute chain
    results = orchestrator.execute_tool_chain(plan, {"target": target})

    # Synthesize results
    synthesis_prompt = orchestrator.generate_llm_synthesis_prompt(results, intent)
    final_report = call_llm(synthesis_prompt)

    return {"report": final_report, "execution_summary": orchestrator.get_execution_summary()}
```

## Tool Metadata Schema

When adding new tools, register them with comprehensive metadata:

```python
@register_tool(ToolMetadata(
    # Basic Info
    name="My New Tool",
    function_name="run_my_tool",
    description="What this tool does",
    category=ToolCategory.NETWORK,

    # Parameters
    parameters=[
        ToolParameter(
            name="target",
            type=InputType.IP_ADDRESS,
            description="IP to scan",
            required=True,
            examples=["192.168.1.1"]
        )
    ],

    # Output
    output=ToolOutput(
        format="json",
        schema={"key": "type"},
        description="What the output contains"
    ),

    # LLM Hints
    use_cases=[
        "When to use this tool",
        "Another use case"
    ],
    triggers=["keywords", "that", "suggest", "this", "tool"],
    prerequisites=["Tool That Should Run First"],
    chains_to=["Tool That Can Follow"],

    # Execution
    timeout=60,
    requires_api_key=False,
    is_intrusive=False,
    is_safe=True
))
def run_my_tool(target: str) -> dict:
    # Implementation
    pass
```

## Benefits

âœ… **True LLM Autonomy**: LLM discovers and selects tools based on intent, not hardcoded logic
âœ… **Extensibility**: Add new tools by just registering metadata
âœ… **Intelligent Chaining**: LLM determines optimal tool sequences
âœ… **Graceful Degradation**: Tool failures don't stop the pipeline
âœ… **Result Synthesis**: LLM aggregates findings into coherent reports
âœ… **Safety Awareness**: LLM knows which tools are intrusive/require authorization
âœ… **Cost Optimization**: LLM avoids redundant tool executions

## Comparison with ClatScope

| Feature | ClatScope | Your System |
|---------|-----------|-------------|
| **Tool Selection** | User menu (manual) | LLM autonomous |
| **Tool Chaining** | Hardcoded in AutoScan | LLM dynamic planning |
| **Execution Order** | Fixed sequence | LLM optimized |
| **Result Synthesis** | Raw data + optional AI fact-check | LLM comprehensive analysis |
| **Extensibility** | Add function + menu entry | Register metadata only |
| **Adaptability** | Static workflows | Dynamic based on findings |

## Next Steps

1. **Test the system**: Run the orchestrator with sample requests
2. **Add more tools**: Register additional security tools with metadata
3. **Integrate with agent**: Add orchestration to your Phase 1 or create AutoScan
4. **Fine-tune prompts**: Optimize LLM prompts for better tool selection
5. **Add parallel execution**: Implement concurrent tool execution for speed

## Advanced Features (Future)

- **Adaptive Depth**: LLM decides when to stop vs continue investigating
- **Rate Limiting**: LLM aware of API quotas and spreads requests
- **Error Recovery**: LLM suggests alternative tools when primary fails
- **Context Tracking**: Maintain investigation context across sessions
- **Risk Assessment**: LLM evaluates risk before running intrusive tools
- **Tool Comparison**: Run multiple tools (e.g., nmap vs masscan) and compare

---

**You now have a foundation for LLM-autonomous security tool orchestration, inspired by ClatScope but with original implementation and true AI autonomy!**
