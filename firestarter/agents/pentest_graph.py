"""LangGraph workflow for pentest agent."""

from typing import Dict, Any, TypedDict, List, Optional, Callable
from langgraph.graph import StateGraph, END
from models.qwen3_agent import Qwen3Agent
from models.functiongemma_agent import FunctionGemmaAgent
from models.deepseek_agent import DeepSeekAgent
from agents.autogen_agents import AutoGenCoordinator
from agents.intent_classifier import IntentClassifier
from agents.direct_answer_agent import DirectAnswerAgent
from tools.executor import get_executor
from rag.results_storage import ToolResultsStorage
from rag.retriever import ConversationRetriever
from websearch.aggregator import SearchAggregator
from knowledge.llamaindex_setup import LlamaIndexKnowledgeBase
from agents.results_qa_agent import ResultsQAAgent
from utils.input_normalizer import InputNormalizer
from memory.manager import get_memory_manager
from agents.context_manager import get_context_manager
from agents.target_clarifier import TargetClarifier


class GraphState(TypedDict):
    """State for LangGraph workflow."""
    user_prompt: str
    analysis: Optional[Dict[str, Any]]
    intent_classification: Optional[Dict[str, Any]]
    direct_answer: Optional[Dict[str, Any]]
    target_clarification: Optional[Dict[str, Any]]  # For ambiguous targets
    subtasks: List[Dict[str, Any]]
    selected_agent: Optional[str]
    tool_results: List[Dict[str, Any]]
    search_results: Optional[Dict[str, Any]]
    knowledge_results: Optional[Dict[str, Any]]
    rag_results: Optional[List[Dict[str, Any]]]
    results_qa_answer: Optional[str]
    final_answer: Optional[str]
    session_id: Optional[str]  # Legacy support
    conversation_id: Optional[str]  # Production: preferred identifier
    conversation_history: List[Dict[str, Any]]


class PentestGraph:
    """LangGraph workflow for pentest agent."""
    
    def __init__(self, 
                 stream_callback: Optional[Callable[[str, str, Any], None]] = None):
        """Initialize pentest graph.
        
        Args:
            stream_callback: Optional callback for streaming events.
                            Called as callback(event_type, event_name, event_data)
        """
        self.qwen3 = Qwen3Agent()
        self.functiongemma = FunctionGemmaAgent()
        self.deepseek = DeepSeekAgent()
        self.autogen = AutoGenCoordinator()
        self.intent_classifier = IntentClassifier()
        self.direct_answer_agent = DirectAnswerAgent()
        self.executor = get_executor()
        self.results_storage = ToolResultsStorage()
        self.conversation_retriever = ConversationRetriever()
        self.search_aggregator = SearchAggregator()
        self.knowledge_base = LlamaIndexKnowledgeBase()
        self.results_qa = ResultsQAAgent()
        self.stream_callback = stream_callback
        self.input_normalizer = InputNormalizer(ai_model=self.qwen3)
        
        # Initialize memory and context managers
        self.memory_manager = get_memory_manager()
        self.context_manager = get_context_manager()
        self.context_manager.set_memory_manager(self.memory_manager)
        
        # Initialize target clarifier
        self.target_clarifier = TargetClarifier(
            functiongemma=self.functiongemma,
            qwen3=self.qwen3,
            memory_manager=self.memory_manager,
            context_manager=self.context_manager,
            stream_callback=self.stream_callback
        )
        
        # Build graph
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Build LangGraph workflow.
        
        Returns:
            Compiled graph
        """
        workflow = StateGraph(GraphState)
        
        # Add nodes
        workflow.add_node("check_target", self._check_target_node)  
        workflow.add_node("detect_confirmation", self._detect_confirmation_node) 
        workflow.add_node("clarify_target", self._clarify_target_node)  
        workflow.add_node("analyze", self._analyze_node)
        workflow.add_node("classify_intent", self._classify_intent_node)
        workflow.add_node("direct_answer", self._direct_answer_node)
        workflow.add_node("select_agent", self._select_agent_node)
        workflow.add_node("execute_tools", self._execute_tools_node)
        workflow.add_node("web_search", self._web_search_node)
        workflow.add_node("knowledge_lookup", self._knowledge_lookup_node)
        workflow.add_node("rag_retrieval", self._rag_retrieval_node)
        workflow.add_node("results_qa", self._results_qa_node)
        workflow.add_node("synthesize", self._synthesize_node)
        
        # Define edges
        workflow.set_entry_point("check_target")
        workflow.add_conditional_edges(
            "check_target",
            self._route_by_target_clarity,
            {
                "clear": "analyze",
                "ambiguous": "detect_confirmation"
            }
        )
        workflow.add_conditional_edges(
            "detect_confirmation",
            self._route_by_confirmation,
            {
                "confirmed": "analyze",
                "not_confirmed": "clarify_target"
            }
        )
        workflow.add_edge("analyze", "classify_intent")
        workflow.add_edge("clarify_target", END)  
        workflow.add_conditional_edges(
            "classify_intent",
            self._route_by_intent,
            {
                "question": "direct_answer",
                "request": "select_agent"
            }
        )
        workflow.add_conditional_edges(
            "direct_answer",
            self._check_answer_sufficient,
            {
                "sufficient": "synthesize",
                "insufficient": "select_agent"
            }
        )
        workflow.add_conditional_edges(
            "select_agent",
            self._should_execute_tools,
            {
                "tools": "execute_tools",
                "search": "web_search",
                "knowledge": "knowledge_lookup",
                "rag": "rag_retrieval",
                "qa": "results_qa",
                "synthesize": "synthesize"
            }
        )
        workflow.add_edge("execute_tools", "synthesize")
        workflow.add_edge("web_search", "synthesize")
        workflow.add_edge("knowledge_lookup", "synthesize")
        workflow.add_edge("rag_retrieval", "synthesize")
        workflow.add_edge("results_qa", "synthesize")
        workflow.add_edge("synthesize", END)
        
        return workflow.compile()
    
    def _analyze_node(self, state: GraphState) -> GraphState:
        """Node 1: Analysis with Qwen3."""
        user_prompt = state["user_prompt"]
        
        # Get comprehensive context from memory manager
        conversation_id = state.get("conversation_id") or state.get("session_id")
        memory_context = self.memory_manager.retrieve_context(
            query=user_prompt,
            k=5,
            session_id=conversation_id,  # Will use conversation_id internally
            include_tool_results=True,
            include_buffer=True
        )
        
        # Format conversation history from buffer (prefer buffer over state)
        conversation_history_str = None
        conversation_buffer = memory_context.get("conversation_buffer", [])
        if conversation_buffer:
            # Use last 5 messages from buffer
            recent_messages = conversation_buffer[-5:]
            # Format with clear context markers
            history_lines = []
            for msg in recent_messages:
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                history_lines.append(f"{role.upper()}: {content}")
            conversation_history_str = "\n".join(history_lines)
            
            # Add context hint if this looks like a continuation
            if len(recent_messages) >= 2:
                last_assistant = None
                for msg in reversed(recent_messages):
                    if msg.get('role') == 'assistant':
                        last_assistant = msg.get('content', '')
                        break
                # Check if last assistant message was asking for clarification
                if last_assistant and any(keyword in last_assistant.lower() for keyword in 
                    ['domain', 'ip address', 'website', 'target', 'clarification', 'provide', 'correct']):
                    # This is likely a continuation - add context hint
                    conversation_history_str = (
                        "CONTEXT: The previous assistant message asked for target clarification. "
                        "The current user message is providing that information. "
                        "This is a CONTINUATION of the pentest request, NOT a new unrelated question.\n\n"
                        + conversation_history_str
                    )
        elif state.get("conversation_history"):
            # Fallback to state conversation_history
            recent_messages = state["conversation_history"][-5:]
            history_lines = []
            for msg in recent_messages:
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                history_lines.append(f"{role.upper()}: {content}")
            conversation_history_str = "\n".join(history_lines)
        
        # Get session context for target information
        session_context = self.context_manager.get_context(state.get("session_context"))
        if session_context and session_context.get_target():
            # Add target context to prompt
            target = session_context.get_target()
            user_prompt = f"{user_prompt}\n\nCurrent target: {target}"
        
        # Create streaming callback for Qwen3
        model_callback = None
        if self.stream_callback:
            def callback(chunk: str):
                self.stream_callback("model_response", "qwen3", chunk)
            model_callback = callback
        
        analysis = self.qwen3.analyze_and_breakdown(
            user_prompt=user_prompt,
            conversation_history=conversation_history_str,
            stream_callback=model_callback
        )
        
        if analysis.get("success"):
            analysis_data = analysis.get("analysis", {})
            state["analysis"] = analysis_data
            state["subtasks"] = analysis_data.get("subtasks", [])
            
            # Update session context with analysis
            if session_context:
                state["session_context"] = session_context.to_dict()
        
        return state
    
    def _check_target_node(self, state: GraphState) -> GraphState:
        """Node: Check if target is ambiguous."""
        # First check if target already verified in session/conversation
        conversation_id = state.get("conversation_id") or state.get("session_id")
        verified_target = self.memory_manager.get_verified_target(
            session_id=conversation_id,
            conversation_id=conversation_id if state.get("conversation_id") else None
        )
        
        if verified_target:
            # Target already verified, mark as clear
            state["target_clarification"] = {
                "is_ambiguous": False,
                "verified_domain": verified_target,
                "has_domain": True,
                "has_ip": False,
                "has_url": False,
                "potential_targets": [verified_target],
                "suggested_questions": [],
                "can_search": False,
                "search_context": {}
            }
            
            # Update session context
            session_context = self.context_manager.get_context()
            if session_context:
                session_context = session_context.merge_with({"target_domain": verified_target})
                state["session_context"] = session_context.to_dict()
            
            return state
        
        # No verified target, check for ambiguity
        user_prompt = state["user_prompt"]
        
        # Get conversation context for semantic understanding
        conversation_context = None
        conversation_history = state.get("conversation_history", [])
        if conversation_history:
            # Get last few messages for context
            recent_messages = conversation_history[-3:] if len(conversation_history) > 3 else conversation_history
            conversation_context = " ".join([msg.get("content", "") for msg in recent_messages if isinstance(msg, dict)])
        
        ambiguity_check = self.input_normalizer.is_target_ambiguous(
            user_prompt, 
            conversation_context=conversation_context
        )
        
        state["target_clarification"] = ambiguity_check
        
        return state
    
    def _clarify_target_node(self, state: GraphState) -> GraphState:
        """Node: Ask user for target clarification or try web search with AI understanding."""
        return self.target_clarifier.clarify_target(state)
    
    def _route_by_target_clarity(self, state: GraphState) -> str:
        """Route based on target clarity.
        
        Returns:
            \"clear\" if target is clear, \"ambiguous\" if needs clarification
        """
        # First check if target already verified
        session_id = state.get("session_id")
        verified_target = self.memory_manager.get_verified_target(session_id)
        
        if verified_target:
            # Target already verified, mark as clear
            clarification = state.get("target_clarification", {})
            clarification["is_ambiguous"] = False
            clarification["verified_domain"] = verified_target
            state["target_clarification"] = clarification
            
            # Update session context
            session_context = self.context_manager.get_context()
            if session_context:
                session_context = session_context.merge_with({"target_domain": verified_target})
                state["session_context"] = session_context.to_dict()
            
            return "clear"
        
        # Check ambiguity
        clarification = state.get("target_clarification", {})
        is_ambiguous = clarification.get("is_ambiguous", False)
        
        return "ambiguous" if is_ambiguous else "clear"
    
    def _detect_confirmation_node(self, state: GraphState) -> GraphState:
        """Node: Detect user confirmation responses.
        
        Checks if user is confirming a previously suggested domain.
        """
        user_prompt = state["user_prompt"].lower().strip()
        conversation_history = state.get("conversation_history", [])
        
        # Confirmation keywords
        confirmation_keywords = [
            "yes", "correct", "right", "that's right", "that is correct",
            "yes it is", "yes it's", "yes its", "yep", "yeah", "ok", "okay",
            "confirmed", "confirm", "that's it", "that is it", "exactly"
        ]
        
        # Check if user input is a confirmation
        is_confirmation = any(keyword in user_prompt for keyword in confirmation_keywords)
        
        if not is_confirmation:
            # Not a confirmation, continue to clarify
            return state
        
        # Look for previously suggested domain in conversation history
        # Check last assistant message for domain suggestions
        suggested_domain = None
        for msg in reversed(conversation_history):
            if msg.get("role") == "assistant":
                content = msg.get("content", "")
                # Look for domain pattern in assistant's last message
                import re
                domain_pattern = r'\b([a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.(?:[a-zA-Z]{2,}|co\.(?:za|uk|jp|kr|nz|au)))\b'
                matches = re.findall(domain_pattern, content)
                if matches:
                    # Take the first domain found (most likely the suggested one)
                    suggested_domain = matches[0]
                    break
        
        # Also check target_clarification for verified_domain
        clarification = state.get("target_clarification", {})
        if not suggested_domain:
            suggested_domain = clarification.get("verified_domain")
        
        if suggested_domain:
            # User confirmed the domain
            conversation_id = state.get("conversation_id") or state.get("session_id")
            
            # Save verified target
            self.memory_manager.save_verified_target(
                session_id=conversation_id,
                domain=suggested_domain,
                conversation_id=conversation_id if state.get("conversation_id") else None
            )
            
            # Update session context
            session_context = self.context_manager.get_context()
            if session_context:
                session_context = session_context.merge_with({"target_domain": suggested_domain})
                state["session_context"] = session_context.to_dict()
            
            # Update target clarification
            clarification["is_ambiguous"] = False
            clarification["verified_domain"] = suggested_domain
            state["target_clarification"] = clarification
            
            # Update user prompt to include verified domain for analysis
            state["user_prompt"] = f"{state['user_prompt']} {suggested_domain}"
        
        return state
    
    def _route_by_confirmation(self, state: GraphState) -> str:
        """Route based on confirmation detection.
        
        Returns:
            \"confirmed\" if user confirmed, \"not_confirmed\" otherwise
        """
        clarification = state.get("target_clarification", {})
        verified_domain = clarification.get("verified_domain")
        
        if verified_domain:
            return "confirmed"
        
        return "not_confirmed"
    
    def _classify_intent_node(self, state: GraphState) -> GraphState:
        """Node: Intent Classification."""
        user_prompt = state["user_prompt"]
        
        # Include conversation history for better intent detection
        conversation_history = state.get("conversation_history", [])
        context_prompt = user_prompt
        if conversation_history:
            # Add last 3 messages for context
            recent = conversation_history[-3:]
            context_lines = [f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in recent]
            context_prompt = f"{user_prompt}\n\nPrevious conversation:\n" + "\n".join(context_lines)
        
        classification = self.intent_classifier.classify(context_prompt)
        state["intent_classification"] = classification
        return state
    
    def _direct_answer_node(self, state: GraphState) -> GraphState:
        """Node: Direct Answer (for questions)."""
        user_prompt = state["user_prompt"]
        conversation_id = state.get("conversation_id") or state.get("session_id")
        
        # Get comprehensive context from memory manager (includes buffer)
        memory_context = self.memory_manager.retrieve_context(
            query=user_prompt,
            k=5,
            session_id=conversation_id,  # Will use conversation_id internally
            include_tool_results=True,
            include_buffer=True
        )
        
        # Retrieve RAG context (semantic search)
        rag_context = memory_context.get("conversation_context", [])
        
        # Get knowledge base results if available
        knowledge_results = {}
        analysis = state.get("analysis", {})
        if analysis:
            knowledge_queries = analysis.get("resources", {}).get("knowledge_queries", {})
            for kb_type, queries in knowledge_queries.items():
                for query in queries[:2]:  # Limit queries
                    result = self.knowledge_base.query(query, index_type=kb_type, top_k=2)
                    if kb_type not in knowledge_results:
                        knowledge_results[kb_type] = []
                    if result.get("success"):
                        knowledge_results[kb_type].append(result)
        
        # Get web search results if needed
        search_results = None
        if analysis:
            queries = analysis.get("resources", {}).get("web_search_queries", [])
            if queries:
                search_results = self.search_aggregator.search_multiple_queries(queries[:2], num_results=3)
        
        # Create streaming callback
        model_callback = None
        if self.stream_callback:
            def callback(chunk: str):
                self.stream_callback("model_response", "direct_answer", chunk)
            model_callback = callback
        
        # Get conversation history from buffer (prefer buffer)
        conversation_history = memory_context.get("conversation_buffer", [])
        if not conversation_history:
            conversation_history = state.get("conversation_history", [])
        
        # Get direct answer
        answer_result = self.direct_answer_agent.answer_question(
            question=user_prompt,
            rag_results=rag_context,
            knowledge_results=knowledge_results if knowledge_results else None,
            search_results=search_results,
            conversation_history=conversation_history,
            stream_callback=model_callback
        )
        
        state["direct_answer"] = answer_result
        state["rag_results"] = rag_context
        state["knowledge_results"] = knowledge_results if knowledge_results else None
        state["search_results"] = search_results
        
        return state
    
    def _route_by_intent(self, state: GraphState) -> str:
        """Route based on intent classification.
        
        Args:
            state: Graph state
            
        Returns:
            Route key ("question" or "request") that maps to node name
        """
        try:
            classification = state.get("intent_classification", {})
            if not classification:
                # Default to request if classification failed
                return "request"
            
            intent = classification.get("intent", "question")
            
            # Return the key that maps to the node in conditional_edges
            if intent == "question":
                return "question"  # Maps to "direct_answer" node
            else:
                return "request"  # Maps to "select_agent" node
        except Exception:
            # Fallback to request on error
            return "request"
    
    def _check_answer_sufficient(self, state: GraphState) -> str:
        """Check if direct answer is sufficient.
        
        Args:
            state: Graph state
            
        Returns:
            "sufficient" or "insufficient"
        """
        direct_answer = state.get("direct_answer")
        if not direct_answer or not isinstance(direct_answer, dict):
            return "insufficient"
        
        sufficient = direct_answer.get("sufficient", False)
        needs_tools = direct_answer.get("needs_tools", False)
        
        if sufficient and not needs_tools:
            return "sufficient"
        else:
            return "insufficient"
    
    def _select_agent_node(self, state: GraphState) -> GraphState:
        """Node 2: AutoGen Agent Selection."""
        analysis = state.get("analysis", {})
        task_type = analysis.get("analysis", {}).get("task_type", "mixed")
        
        agent_name = self.autogen.route_task(state["user_prompt"], task_type)
        state["selected_agent"] = agent_name
        
        return state
    
    def _should_execute_tools(self, state: GraphState) -> str:
        """Conditional routing based on subtasks."""
        subtasks = state.get("subtasks", [])
        
        for subtask in subtasks:
            subtask_type = subtask.get("type", "")
            if subtask_type == "tool_execution":
                return "tools"
            elif subtask_type == "web_search":
                return "search"
            elif subtask_type == "knowledge_lookup":
                return "knowledge"
            elif subtask_type == "rag_retrieval":
                return "rag"
        
        # Check if user is asking about results
        if "result" in state["user_prompt"].lower() or "kết quả" in state["user_prompt"].lower():
            return "qa"
        
        return "synthesize"
    
    def _execute_tools_node(self, state: GraphState) -> GraphState:
        """Node 4: Tool Execution."""
        subtasks = state.get("subtasks", [])
        tool_results = []
        
        # Create streaming callbacks
        model_callback = None
        tool_stream_callback = None
        
        if self.stream_callback:
            def model_cb(chunk: str):
                self.stream_callback("model_response", "functiongemma", chunk)
            model_callback = model_cb
            
            def tool_cb(tool_name: str, command_name: str, line: str):
                self.stream_callback("tool_output", f"{tool_name}:{command_name}" if command_name else tool_name, line)
            tool_stream_callback = tool_cb
        
        for subtask in subtasks:
            if subtask.get("type") == "tool_execution":
                tools = subtask.get("required_tools", [])
                subtask_description = subtask.get("description", "")
                subtask_name = subtask.get("name", "")
                
                # Build context-rich prompt for FunctionGemma
                # Include original user prompt, subtask description, and target information
                user_prompt_original = state.get("user_prompt", "")
                
                # Get verified target from session context or extract from prompt
                session_context = self.context_manager.get_context(state.get("session_context"))
                verified_target = None
                if session_context:
                    verified_target = session_context.get_target()
                
                # Extract targets from user prompt using input normalizer
                from utils.input_normalizer import InputNormalizer
                normalizer = InputNormalizer()
                normalized = normalizer.normalize_input(user_prompt_original, verify_domains=False)
                targets = normalized.get("targets", [])
                
                # Prefer verified target if available
                if verified_target and verified_target not in targets:
                    targets.insert(0, verified_target)
                
                targets_str = ", ".join(targets) if targets else "target from user request"
                
                # Include conversation context in tool prompt
                conversation_context = ""
                conversation_history = state.get("conversation_history", [])
                if conversation_history:
                    recent = conversation_history[-3:]
                    conversation_context = "\n".join([
                        f"{msg.get('role', 'unknown')}: {msg.get('content', '')[:200]}" 
                        for msg in recent
                    ])
                
                for tool_name in tools:
                    # Build comprehensive prompt for FunctionGemma with context
                    tool_prompt = f"""Execute {tool_name} for the following task:
Task: {subtask_name}
Description: {subtask_description}
Target: {targets_str}
Original request: {user_prompt_original}"""
                    
                    if conversation_context:
                        tool_prompt += f"\n\nRecent conversation context:\n{conversation_context}"
                    
                    tool_prompt += "\n\nExtract the required parameters from the context above and execute the tool with appropriate values. For example, if the target is a domain like \"hellogroup.co.za\", use it as the \"domain\" parameter."""
                    
                    result = self.functiongemma.call_with_tools(
                        user_prompt=tool_prompt,
                        agent=state.get("selected_agent"),
                        session_id=state.get("conversation_id") or state.get("session_id"),
                        conversation_history=state.get("conversation_history", []),
                        stream_callback=model_callback,
                        tool_stream_callback=tool_stream_callback
                    )
                    
                    if result.get("tool_results"):
                        for tr in result["tool_results"]:
                            exec_result = tr.get("result", {})
                            tool_results.append(exec_result)
                            
                            # Store in results storage
                            if exec_result.get("success"):
                                self.results_storage.store_result(
                                    tool_name=exec_result.get("tool_name", ""),
                                    parameters=exec_result.get("parameters", {}),
                                    results=exec_result.get("results"),
                                    agent=state.get("selected_agent"),
                                    session_id=state.get("conversation_id") or state.get("session_id"),
                                    conversation_id=state.get("conversation_id"),
                                    execution_id=exec_result.get("execution_id")
                                )
                                
                                # Update agent context with findings from tool results
                                results = exec_result.get("results", {})
                                if isinstance(results, dict):
                                    # Extract findings from results
                                    findings = {}
                                    if "subdomains" in results:
                                        findings["subdomains"] = results["subdomains"]
                                    if "ips" in results:
                                        findings["ips"] = results["ips"]
                                    if "open_ports" in results:
                                        findings["open_ports"] = results["open_ports"]
                                    if "vulnerabilities" in results:
                                        findings["vulnerabilities"] = results["vulnerabilities"]
                                    if "technologies" in results:
                                        findings["technologies"] = results["technologies"]
                                    
                                    if findings:
                                        self.memory_manager.update_agent_context(findings)
                                        # Update session context
                                        self.context_manager.update_context(findings)
        
        state["tool_results"] = tool_results
        return state
    
    def _web_search_node(self, state: GraphState) -> GraphState:
        """Node 5: Web Search."""
        analysis = state.get("analysis", {})
        queries = analysis.get("resources", {}).get("web_search_queries", [])
        
        if queries:
            search_result = self.search_aggregator.search_multiple_queries(queries)
            state["search_results"] = search_result
        
        return state
    
    def _knowledge_lookup_node(self, state: GraphState) -> GraphState:
        """Node 6: LlamaIndex Retrieval."""
        analysis = state.get("analysis", {})
        knowledge_queries = analysis.get("resources", {}).get("knowledge_queries", {})
        
        knowledge_results = {}
        for kb_type, queries in knowledge_queries.items():
            for query in queries:
                result = self.knowledge_base.query(query, index_type=kb_type)
                if kb_type not in knowledge_results:
                    knowledge_results[kb_type] = []
                knowledge_results[kb_type].append(result)
        
        state["knowledge_results"] = knowledge_results
        return state
    
    def _rag_retrieval_node(self, state: GraphState) -> GraphState:
        """Node 7: RAG Retrieval."""
        conversation_id = state.get("conversation_id") or state.get("session_id")
        context = self.conversation_retriever.retrieve_context(
            query=state["user_prompt"],
            session_id=conversation_id,
            conversation_id=state.get("conversation_id")
        )
        state["rag_results"] = context
        return state
    
    def _results_qa_node(self, state: GraphState) -> GraphState:
        """Node 8: Results Q&A."""
        conversation_id = state.get("conversation_id") or state.get("session_id")
        answer = self.results_qa.answer_question(
            question=state["user_prompt"],
            session_id=conversation_id  # ResultsQAAgent will be updated separately
        )
        state["results_qa_answer"] = answer.get("answer", "")
        return state
    
    def _synthesize_node(self, state: GraphState) -> GraphState:
        """Node 9: Synthesis."""
        # Check if we already have a direct answer
        direct_answer = state.get("direct_answer")
        if direct_answer and isinstance(direct_answer, dict):
            if direct_answer.get("sufficient") and direct_answer.get("answer"):
                # Use direct answer if sufficient
                state["final_answer"] = direct_answer.get("answer", "")
                return state
        
        # Otherwise, synthesize from all sources
        synthesis_input = {
            "tool_results": state.get("tool_results", []),
            "search_results": state.get("search_results", {}),
            "knowledge_results": state.get("knowledge_results", {}),
            "rag_results": state.get("rag_results", []),
            "results_qa": state.get("results_qa_answer"),
            "direct_answer": direct_answer.get("answer") if (direct_answer and isinstance(direct_answer, dict)) else None
        }
        
        # Create streaming callback for DeepSeek
        model_callback = None
        if self.stream_callback:
            def callback(chunk: str):
                self.stream_callback("model_response", "deepseek", chunk)
            model_callback = callback
        
        answer = self.deepseek.synthesize_answer(
            user_question=state["user_prompt"],
            search_results=synthesis_input,
            stream_callback=model_callback
        )
        
        state["final_answer"] = answer.get("answer", "")
        return state
    
    def run(self, user_prompt: str, session_id: Optional[str] = None, conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Run workflow (non-streaming).
        
        Args:
            user_prompt: User prompt
            session_id: Session identifier (legacy)
            conversation_id: Conversation identifier (preferred)
            
        Returns:
            Final result
        """
        initial_state: GraphState = {
            "user_prompt": user_prompt,
            "analysis": None,
            "intent_classification": None,
            "direct_answer": None,
            "subtasks": [],
            "selected_agent": None,
            "tool_results": [],
            "search_results": None,
            "knowledge_results": None,
            "rag_results": None,
            "results_qa_answer": None,
            "final_answer": None,
            "session_id": session_id,  # Legacy
            "conversation_id": conversation_id or session_id,  # Production
            "conversation_history": []
        }
        
        final_state = self.graph.invoke(initial_state)
        
        return {
            "success": True,
            "answer": final_state.get("final_answer", ""),
            "tool_results": final_state.get("tool_results", []),
            "search_results": final_state.get("search_results"),
            "knowledge_results": final_state.get("knowledge_results"),
            "session_id": session_id
        }
    
    def run_streaming(self, user_prompt: str, session_id: Optional[str] = None, conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Run workflow with streaming.
        
        Args:
            user_prompt: User prompt
            session_id: Session identifier (legacy)
            conversation_id: Conversation identifier (preferred)
            
        Returns:
            Final result
        """
        # Load conversation history from memory manager
        conversation_history = []
        verified_target = None
        session_context_dict = None
        
        # Prefer conversation_id over session_id
        conv_id = conversation_id or session_id
        
        if conv_id:
            # Get conversation buffer (short-term memory)
            conversation_history = self.memory_manager.get_conversation_buffer(
                session_id=conv_id,
                conversation_id=conversation_id
            )
            
            # Get verified target
            verified_target = self.memory_manager.get_verified_target(
                session_id=conv_id,
                conversation_id=conversation_id
            )
            
            # Get session context
            session_context = self.context_manager.get_context()
            if verified_target and session_context:
                # Update session context with verified target
                session_context = session_context.merge_with({"target_domain": verified_target})
                session_context_dict = session_context.to_dict()
            elif session_context:
                session_context_dict = session_context.to_dict()
        
        initial_state: GraphState = {
            "user_prompt": user_prompt,
            "analysis": None,
            "intent_classification": None,
            "direct_answer": None,
            "target_clarification": None,
            "subtasks": [],
            "selected_agent": None,
            "tool_results": [],
            "search_results": None,
            "knowledge_results": None,
            "rag_results": None,
            "results_qa_answer": None,
            "final_answer": None,
            "session_id": session_id,  # Legacy
            "conversation_id": conversation_id or session_id,  # Production
            "conversation_history": conversation_history,
            "session_context": session_context_dict
        }
        
        final_state = None
        
        # Stream graph execution
        for event in self.graph.stream(initial_state):
            # event is a dict with node names as keys
            for node_name, node_state in event.items():
                if self.stream_callback:
                    self.stream_callback("state_update", node_name, node_state)
                final_state = node_state
        
        if final_state is None:
            final_state = initial_state
        
        return {
            "success": True,
            "answer": final_state.get("final_answer", ""),
            "tool_results": final_state.get("tool_results", []),
            "search_results": final_state.get("search_results"),
            "knowledge_results": final_state.get("knowledge_results"),
            "session_id": session_id
        }